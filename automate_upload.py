import os
import time
import json
import re
import io
import zipfile
import shutil
import cloudscraper
import subprocess
import sys
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© ---
TARGET_BATCH_SIZE_MB = 8
MAX_BATCH_SIZE_MB = 9
TARGET_BATCH_SIZE_BYTES = TARGET_BATCH_SIZE_MB * 1024 * 1024
MAX_BATCH_SIZE_BYTES = MAX_BATCH_SIZE_MB * 1024 * 1024
DEFAULT_BASE_URL = "http://127.0.0.1:7860"

def start_background_services():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ± ÙˆØ§Ù„Ù†ÙÙ‚ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    print("--- [1/2] Ø¬Ø§Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø³ÙŠØ±ÙØ± Evoars... ---")
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµØ­ÙŠØ­
    project_dir = os.path.dirname(os.path.abspath(__file__))
    if "Evoars-main" not in project_dir:
        project_dir = os.path.join(project_dir, "Evoars_local", "Evoars-main")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
    server_proc = subprocess.Popen([sys.executable, "app.py"], cwd=project_dir)
    print("â³ Ù†Ù†ØªØ¸Ø± 10 Ø«ÙˆØ§Ù†Ù Ù„ÙŠØªÙØ¹Ù„ Ø§Ù„Ø³ÙŠØ±ÙØ±...")
    time.sleep(10)
    
    print("--- [2/2] Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø§Ø¨Ø· SSH Tunnel... ---")
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†ÙÙ‚
    tunnel_cmd = "ssh -R 80:127.0.0.1:7860 nokey@localhost.run"
    subprocess.Popen(tunnel_cmd, shell=True)
    
    print("\nâœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªØ´ØºÙŠÙ„. ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø´Ø§Ø´Ø© Ù„Ù†Ø³Ø® Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù….")
    print("Ù…Ù„Ø§Ø­Ø¸Ø©: Ø¥Ø°Ø§ ÙƒÙ†Øª ÙÙŠ CodespacesØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø§Ø¨Ø· Ø§Ù„Ù€ Ports Ø§Ù„Ø«Ø§Ø¨Øª Ø£ÙŠØ¶Ø§Ù‹.\n")

def process_batch(batch, batch_idx, base_url, process_url, output_dir, valid_extensions):
    """Ø±ÙØ¹ Ø§Ù„Ø¯ÙØ¹Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ"""
    scraper = cloudscraper.create_scraper()
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒÙ„ÙŠ Ù„Ù„Ø¯ÙØ¹Ø© Ù„Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    total_size = sum(os.path.getsize(f) for f in batch)
    print(f"\n--- [Ø¯ÙØ¹Ø© {batch_idx}] Ø§Ù„Ø­Ø¬Ù…: {total_size/(1024*1024):.2f}MB | Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±: {len(batch)} ---")
    
    data = {'operation': 'colorize'}
    success = False
    
    for attempt in range(3):
        files = []
        try:
            for img_path in batch:
                files.append(('images', (os.path.basename(img_path), open(img_path, 'rb'), 'image/jpeg')))
            
            if not files: return True

            print(f"â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø±ÙØ¹ ÙˆØ§Ù„Ø§Ù†ØªØ¸Ø§Ø±...")
            response = scraper.post(process_url, data=data, files=files, timeout=600)
            
            for _, file_info in files: file_info[1].close()

            if response.status_code == 200:
                result = response.json()
                download_path = result.get('zip_download_url') or result.get('download_url')
                
                if download_path:
                    # ØªÙˆØ­ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø±Ø§Ø¨Ø· (Ù…Ø·Ù„Ù‚ Ø£Ùˆ Ù†Ø³Ø¨ÙŠ)
                    if download_path.startswith('/'):
                        download_url = f"{base_url.rstrip('/')}{download_path}"
                    elif not download_path.startswith('http'):
                        download_url = f"{base_url.rstrip('/')}/{download_path}"
                    else:
                        download_url = download_path
                    
                    print(f"âœ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙ…Øª! Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ ZIP...")
                    r_download = scraper.get(download_url)
                    
                    if r_download.status_code == 200:
                        with zipfile.ZipFile(io.BytesIO(r_download.content)) as z:
                            for member in z.infolist():
                                if member.filename.lower().endswith(valid_extensions):
                                    filename = os.path.basename(member.filename)
                                    target_path = os.path.join(output_dir, filename)
                                    with open(target_path, "wb") as target:
                                        with z.open(member) as source:
                                            shutil.copyfileobj(source, target)
                        print(f"ğŸ‰ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx}.")
                        success = True
                        break 
                else:
                    print(f"âš ï¸ ÙØ´Ù„: Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø±Ø§Ø¨Ø· ØªØ­Ù…ÙŠÙ„.")
            else:
                print(f"âŒ Ø®Ø·Ø£ Ù…Ù† Ø§Ù„Ø³ÙŠØ±ÙØ± ÙƒÙˆØ¯ {response.status_code}")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ: {e}")
            for _, file_info in files:
                if not file_info[1].closed: file_info[1].close()
        
        if not success: time.sleep(10)
    return success

import threading
HISTORY_FILE = "history.json"
history_lock = threading.Lock()

def load_history():
    if not os.path.exists(HISTORY_FILE): return {"colored": []}
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f: return json.load(f)
    except: return {"colored": []}

def add_to_history(category, item):
    with history_lock:
        history = load_history()
        if item not in history.get(category, []):
            if category not in history: history[category] = []
            history[category].append(item)
            with open(HISTORY_FILE, "w", encoding="utf-8") as f:
                json.dump(history, f, ensure_ascii=False, indent=4)

def colorize_chapter(source_dir, output_dir, base_url):
    chapter_name = os.path.basename(source_dir)
    if not os.path.exists(source_dir): return print(f"Ø®Ø·Ø£: Ø§Ù„Ù…Ø¬Ù„Ø¯ {source_dir} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)
    
    valid_extensions = ('.jpg', '.jpeg', '.png', '.webp')
    image_files = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if f.lower().endswith(valid_extensions)]
    if not image_files: return print(f"ØªÙ†Ø¨ÙŠÙ‡: Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØµÙˆØ±.")

    # ÙØ±Ø² Ø§Ù„ØµÙˆØ± Ø±Ù‚Ù…ÙŠØ§Ù‹
    image_files.sort(key=lambda x: int(re.search(r'(\d+)', os.path.basename(x)).group(1)) if re.search(r'(\d+)', os.path.basename(x)) else 0)
    
    # --- Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø°ÙƒÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù… (8MB - 9MB) ---
    batches = []
    current_batch = []
    current_size = 0
    
    for img_path in image_files:
        size = os.path.getsize(img_path)
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø³ØªØªØ®Ø·Ù‰ Ø§Ù„Ù€ 9 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØªØŒ Ù†ØºÙ„Ù‚ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        if current_size + size > MAX_BATCH_SIZE_BYTES and current_batch:
            batches.append(current_batch)
            current_batch = [img_path]
            current_size = size
        else:
            current_batch.append(img_path)
            current_size += size
            # Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ù€ 8 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ø¨Ø§Ù„Ø¶Ø¨Ø· Ø£Ùˆ Ø£ÙƒØ«Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ (ÙˆÙ„ÙƒÙ† Ø£Ù‚Ù„ Ù…Ù† 9)ØŒ Ù†ÙØ¶Ù„ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚
            if current_size >= TARGET_BATCH_SIZE_BYTES:
                batches.append(current_batch)
                current_batch = []
                current_size = 0
                
    if current_batch:
        batches.append(current_batch)
    
    print(f"ğŸš€ ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ {len(batches)} Ø¯ÙØ¹Ø§Øª (Ø¨Ù…ØªÙˆØ³Ø· 8 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª Ù„Ù„Ø¯ÙØ¹Ø©).")
    
    process_url = f"{base_url.rstrip('/')}/process"
    success_all = True
    
    with ThreadPoolExecutor(max_workers=1) as executor:
        futures = {executor.submit(process_batch, batch, idx, base_url, process_url, output_dir, valid_extensions): idx for idx, batch in enumerate(batches, 1)}
        for future in as_completed(futures):
            if not future.result(): success_all = False

    if success_all:
        print(f"\nâœ… Ù…Ø¨Ø±ÙˆÙƒ! Ø§ÙƒØªÙ…Ù„ ØªÙ„ÙˆÙŠÙ† Ø§Ù„ÙØµÙ„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„.")
        add_to_history("colored", chapter_name)
    return success_all

if __name__ == "__main__":
    print("--- [Ù†Ø¸Ø§Ù… Evoars Ø§Ù„Ø´Ø§Ù…Ù„: ØªØ´ØºÙŠÙ„ ÙˆØªÙ„ÙˆÙŠÙ† ØªÙ„Ù‚Ø§Ø¦ÙŠ] ---")
    
    # Ø³Ø¤Ø§Ù„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª
    choice = input("Ù‡Ù„ ØªØ±ÙŠØ¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ± ÙˆØ§Ù„Ù†ÙÙ‚ Ø§Ù„Ø¢Ù†ØŸ (y/n): ").strip().lower()
    if choice == 'y':
        start_background_services()
    
    usr_url = input(f"Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ± Ø§Ù„Ø¹Ø§Ù… (Ø§Ø¶ØºØ· Enter Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… {DEFAULT_BASE_URL}): ").strip()
    target_url = usr_url if usr_url else DEFAULT_BASE_URL
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¨ÙŠØ¦Ø© (Codespaces Ø£Ùˆ Local)
    is_codespace = os.path.exists("/workspaces")
    if is_codespace:
        base_path = "/workspaces/MANGA-color"
        src_folder = os.path.join(base_path, "source_images")
        out_folder = os.path.join(base_path, "results")
    else:
        src_folder = r"C:\Users\abdob\Desktop\mang\source_images"
        out_folder = r"C:\Users\abdob\Desktop\mang\results"
    
    if not os.path.exists(src_folder): os.makedirs(src_folder, exist_ok=True)
    
    colorize_chapter(src_folder, out_folder, target_url)



