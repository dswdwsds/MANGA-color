import os
import time
import json
import re
import io
import zipfile
import shutil
import cloudscraper
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_batch(batch, batch_idx, base_url, process_url, output_dir, valid_extensions):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„ØµÙˆØ±: Ø±ÙØ¹ØŒ Ø§Ù†ØªØ¸Ø§Ø±ØŒ ØªØ­Ù…ÙŠÙ„ØŒ ÙˆÙÙƒ Ø¶ØºØ·.
    """
    scraper = cloudscraper.create_scraper()
    print(f"\n--- [Thread] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx} ({len(batch)} ØµÙˆØ±) ---")
    
    data = {'operation': 'colorize'}
    success = False
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© 3 Ù…Ø±Ø§Øª
    for attempt in range(3):
        files = []
        try:
            for img_path in batch:
                files.append(('images', (os.path.basename(img_path), open(img_path, 'rb'), 'image/jpeg')))
            
            if attempt > 0:
                print(f"[Thread] Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx} (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1})...")

            response = scraper.post(process_url, data=data, files=files, timeout=600)
            
            # Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…Ù„ÙØ§Øª
            for _, file_info in files:
                file_info[1].close()

            if response.status_code == 200:
                result = response.json()
                download_url = result.get('download_url')
                if download_url:
                    if not download_url.startswith('http'):
                        download_url = f"{base_url}{download_url}"
                    
                    print(f"[Thread] ØªÙ… ØªÙ„ÙˆÙŠÙ† Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx} Ø¨Ù†Ø¬Ø§Ø­! Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù…ÙŠÙ„...")
                    r_download = scraper.get(download_url)
                    
                    # ÙÙƒ Ø§Ù„Ø¶ØºØ· Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª (Thread Safe ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ù„Ø£Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø®ØªÙ„ÙØ©)
                    with zipfile.ZipFile(io.BytesIO(r_download.content)) as z:
                        for member in z.infolist():
                            if member.filename.lower().endswith(valid_extensions):
                                filename = os.path.basename(member.filename)
                                source = z.open(member)
                                target_path = os.path.join(output_dir, filename)
                                with open(target_path, "wb") as target:
                                    shutil.copyfileobj(source, target)
                    print(f"âœ… [Thread] ØªÙ… Ø­ÙØ¸ ØµÙˆØ± Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx}.")
                    success = True
                    break 
                else:
                    print(f"âš ï¸ [Thread] ÙØ´Ù„ Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx}: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø§Ø¨Ø· ØªØ­Ù…ÙŠÙ„.")
            else:
                print(f"âš ï¸ [Thread] ÙØ´Ù„ Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx} Ø¨ÙƒÙˆØ¯: {response.status_code}")
            
        except Exception as e:
            print(f"âŒ [Thread] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx} (Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}): {e}")
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…Ù„ÙØ§Øª
            for _, file_info in files:
                if not file_info[1].closed:
                    file_info[1].close()
        
        if attempt < 2:
            time.sleep(5) # Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø¹Ø§Ø¯Ø©

    return success

def colorize_chapter(source_dir, output_dir):
    """
    ØªÙ‚ÙˆÙ… Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø±ÙØ¹ Ø§Ù„ØµÙˆØ± Ù…Ù† Ù…Ø¬Ù„Ø¯ Ù…Ø¹ÙŠÙ† Ù„ØªÙ„ÙˆÙŠÙ†Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ (Parallel Processing).
    """
    if not os.path.exists(source_dir):
        print(f"Ø®Ø·Ø£: Ø§Ù„Ù…Ø¬Ù„Ø¯ {source_dir} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")
        return False

    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    valid_extensions = ('.jpg', '.jpeg', '.png', '.webp')
    image_files = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if f.lower().endswith(valid_extensions)]
    
    if not image_files:
        print(f"ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ØµÙˆØ± ÙÙŠ {source_dir}")
        return False

    # ÙØ±Ø² Ø§Ù„ØµÙˆØ±
    image_files.sort(key=lambda x: int(re.search(r'(\d+)', os.path.basename(x)).group(1)) if re.search(r'(\d+)', os.path.basename(x)) else 0)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª (3 ØµÙˆØ± Ù„ÙƒÙ„ Ø¯ÙØ¹Ø©)
    batch_size = 3
    batches = [image_files[i:i + batch_size] for i in range(0, len(image_files), batch_size)]
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] ØªÙ… ØªÙ‚Ø³ÙŠÙ… {len(image_files)} ØµÙˆØ±Ø© Ø¥Ù„Ù‰ {len(batches)} Ø¯ÙØ¹Ø§Øª.")
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ© (3 Ø¯ÙØ¹Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯)...")

    base_url = "https://koesan-mangaspaces.hf.space"
    process_url = f"{base_url}/process"
    
    success_all = True
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ (Ø¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ 3 Ø®ÙŠÙˆØ·)
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(process_batch, batch, idx, base_url, process_url, output_dir, valid_extensions): idx for idx, batch in enumerate(batches, 1)}
        
        for future in as_completed(futures):
            batch_idx = futures[future]
            try:
                if not future.result():
                    success_all = False
                    print(f"âŒ ÙØ´Ù„Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx} Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹.")
            except Exception as exc:
                print(f"âŒ Ø­Ø¯Ø« Ø§Ø³ØªØ«Ù†Ø§Ø¡ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© {batch_idx}: {exc}")
                success_all = False

    if success_all:
        print(f"\nâœ… Ø§ÙƒØªÙ…Ù„ ØªÙ„ÙˆÙŠÙ† ÙƒØ§ÙØ© Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: {output_dir}")
    else:
        print(f"\nâš ï¸ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù…Ø¹ ÙˆØ¬ÙˆØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")
        
    return success_all

if __name__ == "__main__":
    src = r"C:\Users\abdob\Desktop\mang\source_images"
    out = r"C:\Users\abdob\Desktop\mang\results"
    colorize_chapter(src, out)

